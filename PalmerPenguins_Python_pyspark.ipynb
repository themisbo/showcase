{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a85904",
   "metadata": {},
   "source": [
    "## Classification of the Palmer penguins data\n",
    "\n",
    "This document is part of the showcase, where I replicate the same brief and simple analyses with different tools.\n",
    "\n",
    "This particular file focuses on simple classification of the Palmer penguins data from the tidytuesday project.\n",
    "\n",
    "The data can be found in <https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-07-28>. They consist of one documents: *penguins.csv* contains information and measurements about some penguins.\n",
    "\n",
    "For the specific analysis I will use **Python** and **pyspark** (plus **Jupyter notebook**).\n",
    "\n",
    "We start by loading the pyspark package and initiating a spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0c04615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762604d4",
   "metadata": {},
   "source": [
    "We load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "708d1ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-28/penguins.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "penguins = spark.read.csv(SparkFiles.get(\"penguins.csv\"), header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f80f656",
   "metadata": {},
   "source": [
    "and check the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bc320ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- species: string (nullable = true)\n",
      " |-- island: string (nullable = true)\n",
      " |-- bill_length_mm: string (nullable = true)\n",
      " |-- bill_depth_mm: string (nullable = true)\n",
      " |-- flipper_length_mm: string (nullable = true)\n",
      " |-- body_mass_g: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "penguins.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a6608b",
   "metadata": {},
   "source": [
    "Everything was read as string, so we have to fix the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e31e303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- species: string (nullable = true)\n",
      " |-- island: string (nullable = true)\n",
      " |-- bill_length_mm: float (nullable = true)\n",
      " |-- bill_depth_mm: float (nullable = true)\n",
      " |-- flipper_length_mm: float (nullable = true)\n",
      " |-- body_mass_g: float (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType,FloatType,StringType,LongType\n",
    "\n",
    "penguins = penguins.withColumn(\"bill_length_mm\",col(\"bill_length_mm\").cast(FloatType())) \\\n",
    "    .withColumn(\"bill_depth_mm\",col(\"bill_depth_mm\").cast(FloatType())) \\\n",
    "    .withColumn(\"flipper_length_mm\",col(\"flipper_length_mm\").cast(FloatType())) \\\n",
    "    .withColumn(\"body_mass_g\",col(\"body_mass_g\").cast(FloatType())) \\\n",
    "    .withColumn(\"year\",col(\"year\").cast(IntegerType()))\n",
    "\n",
    "penguins.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89935976",
   "metadata": {},
   "source": [
    "We can see now one record indicatively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f871d704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------\n",
      " species           | Adelie    \n",
      " island            | Torgersen \n",
      " bill_length_mm    | 39.1      \n",
      " bill_depth_mm     | 18.7      \n",
      " flipper_length_mm | 181.0     \n",
      " body_mass_g       | 3750.0    \n",
      " sex               | male      \n",
      " year              | 2007      \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "penguins.show(1, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06db5b46",
   "metadata": {},
   "source": [
    "And the summary statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a10eb8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------\n",
      " summary           | count              \n",
      " species           | 344                \n",
      " island            | 344                \n",
      " bill_length_mm    | 342                \n",
      " bill_depth_mm     | 342                \n",
      " flipper_length_mm | 342                \n",
      " body_mass_g       | 342                \n",
      " sex               | 344                \n",
      " year              | 344                \n",
      "-RECORD 1-------------------------------\n",
      " summary           | mean               \n",
      " species           | null               \n",
      " island            | null               \n",
      " bill_length_mm    | 43.921929733097905 \n",
      " bill_depth_mm     | 17.151169584508526 \n",
      " flipper_length_mm | 200.91520467836258 \n",
      " body_mass_g       | 4201.754385964912  \n",
      " sex               | null               \n",
      " year              | 2008.0290697674418 \n",
      "-RECORD 2-------------------------------\n",
      " summary           | stddev             \n",
      " species           | null               \n",
      " island            | null               \n",
      " bill_length_mm    | 5.4595837704792185 \n",
      " bill_depth_mm     | 1.9747931679093145 \n",
      " flipper_length_mm | 14.061713679356885 \n",
      " body_mass_g       | 801.9545356980957  \n",
      " sex               | null               \n",
      " year              | 0.8183559254836943 \n",
      "-RECORD 3-------------------------------\n",
      " summary           | min                \n",
      " species           | Adelie             \n",
      " island            | Biscoe             \n",
      " bill_length_mm    | 32.1               \n",
      " bill_depth_mm     | 13.1               \n",
      " flipper_length_mm | 172.0              \n",
      " body_mass_g       | 2700.0             \n",
      " sex               | NA                 \n",
      " year              | 2007               \n",
      "-RECORD 4-------------------------------\n",
      " summary           | max                \n",
      " species           | Gentoo             \n",
      " island            | Torgersen          \n",
      " bill_length_mm    | 59.6               \n",
      " bill_depth_mm     | 21.5               \n",
      " flipper_length_mm | 231.0              \n",
      " body_mass_g       | 6300.0             \n",
      " sex               | male               \n",
      " year              | 2009               \n",
      "\n"
     ]
    }
   ],
   "source": [
    "penguins.describe().show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3cedd3",
   "metadata": {},
   "source": [
    "There is some confusion with missing values, so we are going to replace the 'NA's with \"None\"s, which will turn them to \"null\"s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30c17e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------+-------------+-----------------+-----------+------+----+\n",
      "|species|   island|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|   sex|year|\n",
      "+-------+---------+--------------+-------------+-----------------+-----------+------+----+\n",
      "| Adelie|Torgersen|          39.1|         18.7|            181.0|     3750.0|  male|2007|\n",
      "| Adelie|Torgersen|          39.5|         17.4|            186.0|     3800.0|female|2007|\n",
      "| Adelie|Torgersen|          40.3|         18.0|            195.0|     3250.0|female|2007|\n",
      "| Adelie|Torgersen|          null|         null|             null|       null|  null|2007|\n",
      "| Adelie|Torgersen|          36.7|         19.3|            193.0|     3450.0|female|2007|\n",
      "| Adelie|Torgersen|          39.3|         20.6|            190.0|     3650.0|  male|2007|\n",
      "| Adelie|Torgersen|          38.9|         17.8|            181.0|     3625.0|female|2007|\n",
      "| Adelie|Torgersen|          39.2|         19.6|            195.0|     4675.0|  male|2007|\n",
      "| Adelie|Torgersen|          34.1|         18.1|            193.0|     3475.0|  null|2007|\n",
      "| Adelie|Torgersen|          42.0|         20.2|            190.0|     4250.0|  null|2007|\n",
      "| Adelie|Torgersen|          37.8|         17.1|            186.0|     3300.0|  null|2007|\n",
      "| Adelie|Torgersen|          37.8|         17.3|            180.0|     3700.0|  null|2007|\n",
      "| Adelie|Torgersen|          41.1|         17.6|            182.0|     3200.0|female|2007|\n",
      "| Adelie|Torgersen|          38.6|         21.2|            191.0|     3800.0|  male|2007|\n",
      "| Adelie|Torgersen|          34.6|         21.1|            198.0|     4400.0|  male|2007|\n",
      "| Adelie|Torgersen|          36.6|         17.8|            185.0|     3700.0|female|2007|\n",
      "| Adelie|Torgersen|          38.7|         19.0|            195.0|     3450.0|female|2007|\n",
      "| Adelie|Torgersen|          42.5|         20.7|            197.0|     4500.0|  male|2007|\n",
      "| Adelie|Torgersen|          34.4|         18.4|            184.0|     3325.0|female|2007|\n",
      "| Adelie|Torgersen|          46.0|         21.5|            194.0|     4200.0|  male|2007|\n",
      "+-------+---------+--------------+-------------+-----------------+-----------+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "penguins = penguins.replace('NA', None)\n",
    "penguins.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e942046",
   "metadata": {},
   "source": [
    "Now we can count the missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f534133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------------+-------------+-----------------+-----------+---+----+\n",
      "|species|island|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|sex|year|\n",
      "+-------+------+--------------+-------------+-----------------+-----------+---+----+\n",
      "|      0|     0|             2|            2|                2|          2| 11|   0|\n",
      "+-------+------+--------------+-------------+-----------------+-----------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "penguins.select([count(when(col(c).isNull(), c)).alias(c) for c in penguins.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47378d81",
   "metadata": {},
   "source": [
    "There is a few of them, so we are just going to drop them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a9fe01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------+-------------+-----------------+-----------+------+----+\n",
      "|species|   island|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|   sex|year|\n",
      "+-------+---------+--------------+-------------+-----------------+-----------+------+----+\n",
      "| Adelie|Torgersen|          39.1|         18.7|            181.0|     3750.0|  male|2007|\n",
      "| Adelie|Torgersen|          39.5|         17.4|            186.0|     3800.0|female|2007|\n",
      "| Adelie|Torgersen|          40.3|         18.0|            195.0|     3250.0|female|2007|\n",
      "| Adelie|Torgersen|          36.7|         19.3|            193.0|     3450.0|female|2007|\n",
      "| Adelie|Torgersen|          39.3|         20.6|            190.0|     3650.0|  male|2007|\n",
      "| Adelie|Torgersen|          38.9|         17.8|            181.0|     3625.0|female|2007|\n",
      "| Adelie|Torgersen|          39.2|         19.6|            195.0|     4675.0|  male|2007|\n",
      "| Adelie|Torgersen|          41.1|         17.6|            182.0|     3200.0|female|2007|\n",
      "| Adelie|Torgersen|          38.6|         21.2|            191.0|     3800.0|  male|2007|\n",
      "| Adelie|Torgersen|          34.6|         21.1|            198.0|     4400.0|  male|2007|\n",
      "| Adelie|Torgersen|          36.6|         17.8|            185.0|     3700.0|female|2007|\n",
      "| Adelie|Torgersen|          38.7|         19.0|            195.0|     3450.0|female|2007|\n",
      "| Adelie|Torgersen|          42.5|         20.7|            197.0|     4500.0|  male|2007|\n",
      "| Adelie|Torgersen|          34.4|         18.4|            184.0|     3325.0|female|2007|\n",
      "| Adelie|Torgersen|          46.0|         21.5|            194.0|     4200.0|  male|2007|\n",
      "| Adelie|   Biscoe|          37.8|         18.3|            174.0|     3400.0|female|2007|\n",
      "| Adelie|   Biscoe|          37.7|         18.7|            180.0|     3600.0|  male|2007|\n",
      "| Adelie|   Biscoe|          35.9|         19.2|            189.0|     3800.0|female|2007|\n",
      "| Adelie|   Biscoe|          38.2|         18.1|            185.0|     3950.0|  male|2007|\n",
      "| Adelie|   Biscoe|          38.8|         17.2|            180.0|     3800.0|  male|2007|\n",
      "+-------+---------+--------------+-------------+-----------------+-----------+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "penguins = penguins.na.drop()\n",
    "penguins.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c714377",
   "metadata": {},
   "source": [
    "Now we check again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9d4e718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------------+-------------+-----------------+-----------+---+----+\n",
      "|species|island|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|sex|year|\n",
      "+-------+------+--------------+-------------+-----------------+-----------+---+----+\n",
      "|      0|     0|             0|            0|                0|          0|  0|   0|\n",
      "+-------+------+--------------+-------------+-----------------+-----------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "penguins.select([count(when(col(c).isNull(), c)).alias(c) for c in penguins.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb2d6c5",
   "metadata": {},
   "source": [
    "We also drop the \"year\" column, because it is not going to be useful for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3f4e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = penguins.drop('year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d722c6f1",
   "metadata": {},
   "source": [
    "Now we can split the dataset to training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a4b8a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_penguins, test_penguins = penguins.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4a2881",
   "metadata": {},
   "source": [
    "We add some pre-processing steps, specifically:\n",
    "\n",
    "* StringIndexer will turn the strings into numerical factors\n",
    "* OneHotEncoder will perform one-hot-encoding of the categorical features\n",
    "* StandardScaler will normalize all numeric predictors\n",
    "* VectorAssembler will assemble the features into a feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4883dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802ecbee",
   "metadata": {},
   "source": [
    "We can call the dataset to check again on the attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81b8c0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[species: string, island: string, bill_length_mm: float, bill_depth_mm: float, flipper_length_mm: float, body_mass_g: float, sex: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penguins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaa2859",
   "metadata": {},
   "source": [
    "We start turning all string attributes to factors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1f677a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_Ind = StringIndexer(inputCol=\"sex\", outputCol=\"island\") , StringIndexer(inputCol=\"sex_ind\", outputCol=\"island_ind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50dccd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_indexer = StringIndexer(inputCol=\"sex\", outputCol=\"sex_ind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cba26f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "island_indexer = StringIndexer(inputCol=\"island\", outputCol=\"island_ind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36e939f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_indexer = StringIndexer(inputCol=\"species\", outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588ceb2e",
   "metadata": {},
   "source": [
    "One-hot-encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c94bddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(inputCols = ['sex_ind', 'island_ind'], outputCols=['sex_ohe', 'island_ohe'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63e077e",
   "metadata": {},
   "source": [
    "Assembling all numerical features and scaling them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b43e1fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler1 = VectorAssembler(inputCols=['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g'], outputCol=\"features_scaled1\")\n",
    "scaler = StandardScaler(inputCol=\"features_scaled1\", outputCol=\"features_scaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a1fd35",
   "metadata": {},
   "source": [
    "Assembling all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d440bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler2 = VectorAssembler(inputCols=['sex_ohe', 'island_ohe','features_scaled'], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cc6ad2",
   "metadata": {},
   "source": [
    "The first model we are going to use is a multiclass logistic regression model (Note that spark's version of support vector machines doesnot support more than two classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fad805ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7f4ed7",
   "metadata": {},
   "source": [
    "We construct a pipeline with the pre-processing steps and the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e2b0915",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [sex_indexer, island_indexer, species_indexer, ohe, assembler1, scaler, assembler2, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6ed537",
   "metadata": {},
   "source": [
    "We fit the pipeline into the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "caaa79b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pModel = pipeline.fit(train_penguins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cde75eb",
   "metadata": {},
   "source": [
    "And we use it on the testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0eacae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testingPred = pModel.transform(test_penguins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbac79da",
   "metadata": {},
   "source": [
    "We can check the predictions and their probabilities (remember that we have a multiclass logistic regression model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5624843b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+\n",
      "|label|         probability|prediction|\n",
      "+-----+--------------------+----------+\n",
      "|  0.0|[0.43380747392115...|       0.0|\n",
      "|  0.0|[0.43140546838099...|       0.0|\n",
      "|  0.0|[0.38064482728401...|       1.0|\n",
      "|  0.0|[0.45155213695603...|       0.0|\n",
      "|  0.0|[0.40074319738246...|       0.0|\n",
      "|  0.0|[0.41802236439608...|       0.0|\n",
      "|  0.0|[0.42153372637713...|       0.0|\n",
      "|  0.0|[0.41116176426650...|       0.0|\n",
      "|  0.0|[0.46839620040492...|       0.0|\n",
      "|  0.0|[0.47184560104111...|       0.0|\n",
      "|  0.0|[0.49111323991538...|       0.0|\n",
      "|  0.0|[0.45547413833924...|       0.0|\n",
      "|  0.0|[0.46198566862360...|       0.0|\n",
      "|  0.0|[0.45079245281516...|       0.0|\n",
      "|  0.0|[0.46027711782604...|       0.0|\n",
      "|  0.0|[0.47554251764059...|       0.0|\n",
      "|  0.0|[0.48720154024803...|       0.0|\n",
      "|  0.0|[0.44610865215814...|       0.0|\n",
      "|  0.0|[0.44215089511852...|       0.0|\n",
      "|  0.0|[0.46331029306059...|       0.0|\n",
      "+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testingPred.select('label','probability','prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b037206e",
   "metadata": {},
   "source": [
    "Finally, we can calculate the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "061a559f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8428571428571429"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol = \"label\", predictionCol = \"prediction\", metricName = \"accuracy\")\n",
    "acc = evaluator.evaluate(testingPred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc00b9c2",
   "metadata": {},
   "source": [
    "About 84%.\n",
    "\n",
    "For the second model, we use a decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "270d99c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47da85e5",
   "metadata": {},
   "source": [
    "We construct the new pipeline by combining the old pre-processing steps with the new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c21d5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dt = Pipeline(stages = [sex_indexer, island_indexer, species_indexer, ohe, assembler1, scaler, assembler2, dt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fbbf73",
   "metadata": {},
   "source": [
    "We fit the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ee95fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pModel_dt = pipeline_dt.fit(train_penguins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6a7ec5",
   "metadata": {},
   "source": [
    "And test on the testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78ce7b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "testingPred_dt = pModel_dt.transform(test_penguins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf73948",
   "metadata": {},
   "source": [
    "We can check the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c78156db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       2.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       2.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testingPred_dt.select('label','prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f63c0a",
   "metadata": {},
   "source": [
    "And calculate the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f96e1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9714285714285714"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_dt = evaluator.evaluate(testingPred_dt)\n",
    "acc_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba6d699",
   "metadata": {},
   "source": [
    "The decision tree was correct in 97% of its predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2dca37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
