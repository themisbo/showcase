---
title: "Palmer Penguins"
author: "themisbo"
date: "2/10/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Classification (Bayesian) of the Palmer penguins data

This document is part of the showcase, where I replicate the same brief and simple analyses with different tools.

This particular file focuses on simple (Bayesian multiclass logit) classification of the Palmer penguins data from the tidytuesday project.

The data can be found in <https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-07-28>. They consist of one documents: *penguins.csv* contains information and measurements about some penguins.

For the specific analysis I will use **R** and **rstan** (plus **RMarkdown**).

We start by loading the packages:

```{r}
library(tidyverse) # metapackage for data analysis
library(tidymodels) # metapackage for machine learning
library(rstan) # package for stan interface

theme_set(theme_light()) # global configuration for visualizations
```

and the dataset:

```{r}
penguins <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-28/penguins.csv')
```

We can have a look at the nature of the penguin data:

```{r}
glimpse(penguins)
```

and a few summary statistics:

```{r}
skimr::skim(penguins)
```

Our main goal is to try and build a model that classifies the species of the penguins based on their other characteristics.

We start by dropping the missing values:

```{r}
penguins <- penguins %>%
  drop_na()
```

We are also going to drop the year column and change all character features to factors:

```{r}
penguins_data <- penguins %>%
  select(-year) %>%
  mutate_if(is.character, factor)
```

Now we can split the dataset to training and testing:

```{r}
set.seed(123)
data_split <- initial_split(penguins_data, strata = species)
data_train_sp <- training(data_split)
data_test_sp <- testing(data_split)
```

and specify the recipe (data pre-processing and feature engineering steps).

This is going to include the following steps:
* Get rid of (possible) zero variance predictors
* Normalize all numeric predictors
* Switch all nominal predictors to dummy variables

```{r}
rec <- recipe(species~., data = penguins_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())
```

We apply the pre-processing to training and testing:

```{r}
data_train <- rec %>% prep() %>% bake(new_data = data_train_sp)
data_test <- rec %>% prep() %>% bake(new_data = data_test_sp)
```

Now we specify the multi-logit model in stan:

```{r}
scode <- "
data {
  int<lower = 0> N; //the number of training observations
  int<lower = 0> D; //the number of features
  int<lower = 0> K; //the number of classes
  int y[N]; //the response
  matrix[N,D] x; //the model matrix
}

parameters {
  matrix[D,K] beta; //the regression parameters
}

model {
  matrix[N, K] x_beta = x * beta;
  to_vector(beta) ~ normal(0, 3);

  for (n in 1:N)
    y[n] ~ categorical_logit(x_beta[n]');
}

"
```

and we collect all the important information from the data into a list:

```{r}
dat <- list(N  = data_train %>% count() %>% as.numeric(),
            D    = ncol(data_train) - 1,
            K     = data_train$species %>% unique() %>% length(),
            y      = data_train$species %>% as.numeric(),
            x      = data_train %>% select(-species))
```

We run the stan model with 2 chains, including a warmup period and thinning:

```{r}
resStan <- stan(model_code = scode, data = dat,
                chains = 2, iter = 3000, warmup = 500, thin = 10)
```

We can plot the traceplots to check if the chains mixed well:

```{r}
traceplot(resStan, pars = c("beta"), inc_warmup = TRUE)
```

That looks pretty good. We can also print summary statistics from the posterior:

```{r}
print(resStan, pars = c("beta"))
```

Finally, we can extract the posterior from the stan object:

```{r}
post_object <- extract(resStan)
```

In order to assess the model we need to calculate the predictions for the testing data.
If we wanted to, we could do this for the whole chain, which would give us distributions of the probabilities of each class. For the purposes of this showcase project, we are going to use the MAP (maximum a posteriori estimate), which is effectively the mode of the posterior.

The whole process is:
* We extract the MAP from the posterior sample.
* We calculate the sum of products of parameters and data.
* We take the inverse logit of that quantity.
* We take the argmax of the probabilities of each datapoint.

Finally we can calculate the accuracy as the ratio of species predicted correctly over the number of testing data:

```{r}
l_pred <- function(x, class) {x * post_object$beta[MAP_ind,,class]}
inv_logit <- function(x) {exp(x)/(1+exp(x))}

MAP_ind <- which.max(post_object$lp__)

adelie_logits <- rep(0, data_test %>% count() %>% as.numeric())
for (i in 1:length(adelie_logits)){
  adelie_logits[i] <- sum((data_test %>% select(-species))[i,] * post_object$beta[MAP_ind,,1])
}

gentoo_logits <- rep(0, data_test %>% count() %>% as.numeric())
for (i in 1:length(gentoo_logits)){
  gentoo_logits[i] <- sum((data_test %>% select(-species))[i,] * post_object$beta[MAP_ind,,2])
}

chinstrap_logits <- rep(0, data_test %>% count() %>% as.numeric())
for (i in 1:length(chinstrap_logits)){
  chinstrap_logits[i] <- sum((data_test %>% select(-species))[i,] * post_object$beta[MAP_ind,,3])
}

adelie_probs <- inv_logit(adelie_logits)
gentoo_probs <- inv_logit(gentoo_logits)
chinstrap_probs <- inv_logit(chinstrap_logits)

all_probs <- cbind(adelie_probs,gentoo_probs,chinstrap_probs)

apply(all_probs, 1, which.max)

length(which(apply(all_probs, 1, which.max) == data_test$species %>% as.numeric()) )/length(data_test$species %>% as.numeric())
```

The result was about 98.8% accuracy.
