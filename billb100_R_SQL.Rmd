---
title: "Data analysis with R and SQL"
author: "themisbo"
date: "3/10/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data analysis of the billboard 100 data

This document is part of the showcase, where I replicate the same brief and simple analyses with different tools.

This particular file focuses on data analysis (a few queries) of the billboard 100 data from the tidytuesday project.

The data can be found in <https://github.com/rfordatascience/tidytuesday/tree/master/data/2021/2021-09-14>. They consist of two documents: *billboard.csv* contains information about the songs focusing on their position in the top100 list at different weeks. *audio_features.csv* contains information about specific attributes of the songs from spotify.

For the specific analysis I will use **R** and **dplyr** (plus **RMarkdown**).

We start by loading the packages:

```{r}
library(tidyverse) # metapackage for data analysis, including a way to load the data
library(DBI) # connect to the database
```

and the datsets (In practice the datasets can be directly loaded from the database, but here we are going to load them in R and then fill the information into db tables):

```{r}
billboard <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-14/billboard.csv')

audio_features <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-14/audio_features.csv')
```

We establish the database connection. For this example, we use PostgreSQL:

```{r}
con <- DBI::dbConnect(RPostgres::Postgres(),
                      "tidytuesday_db",
                      user = "postgres",
                      password = rstudioapi::askForPassword("Database password"))
```

We copy the dataframes (tibbles) into database tables:

```{r}
dplyr::copy_to(con, billboard, "billboard", temporary = FALSE)
#dbReadTable(con, "billboard")

dplyr::copy_to(con, audio_features, "audio_features", temporary = FALSE)
#dbReadTable(con, "audio_features")

dbListTables(con)

```

We start with a basic query that corresponds to extracting the first 10 lines of the billboard dataframe:

```{r}
query_base <- "SELECT * FROM billboard LIMIT 10"
res_base <- dbSendQuery(con, query_base)
dbFetch(res_base)
dbClearResult(res_base)
```

For the first main query, our aim is to select only the songs that have reached the No 1 spot of the billboard and see how many weeks they have stayed at the billboard in total:

```{r}
query_top10 <- "
SELECT performer, song, MAX(weeks_on_chart) AS max_weeks
  FROM billboard
  WHERE (peak_position=1.0)
  GROUP BY performer, song
  ORDER BY max_weeks DESC
  LIMIT 10
"
res_top10 <- dbSendQuery(con, query_top10)
dbFetch(res_top10)
dbClearResult(res_top10)
```

For the second main query, our aim is to derive information about the peak position a song has reached in the billboard and the main spotify information. For this, we need to join the two datasets:

```{r}
query_merge <- "
SELECT performer, song, bill_sub.song_id, audio_sub.song_id, best_position, spotify_genre
  FROM 
    (SELECT performer, song, song_id, MAX(peak_position) AS best_position
      FROM billboard
      GROUP BY performer, song, song_id) AS bill_sub
INNER JOIN 
    (SELECT song_id, spotify_genre, danceability, energy, key, loudness, speechiness, acousticness, instrumentalness, liveness, valence, tempo
    FROM audio_features) AS audio_sub
ON bill_sub.song_id = audio_sub.song_id
ORDER BY performer
LIMIT 10
"
res_merge <- dbSendQuery(con, query_merge)
dbFetch(res_merge)
dbClearResult(res_merge)
```

Finally, we delete all tables (not really common for standard SQL analysis, but this is just a showcase) and we disconnect from the database.

```{r}
dbRemoveTable(con, "billboard")
dbRemoveTable(con, "audio_features")

dbDisconnect(con)
```

